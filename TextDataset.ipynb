{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.ascii_letters + string.digits\n",
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the dataset:\n",
    "import torch\n",
    "# class for text-datasets\n",
    "# Jama, NEJM and lancet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import unicodedata\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    SPECIAL_CHARS = [None, \"\\n\"]\n",
    "    def __len__(self):\n",
    "        return len(self.total_examples)\n",
    "        pass\n",
    " \n",
    "    # generates tensors for the vocab\n",
    "    # i.e. assigns a number to each character\n",
    "    # Q: possible to do one-pass?\n",
    "    def make_vocab(self):\n",
    "        \n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "        index = 0\n",
    "        with open(self.text_file) as file:\n",
    "            for line in file:\n",
    "                for char in line:\n",
    "                    if char not in self.char2index:\n",
    "                        self.char2index[char] = index\n",
    "                        self.index2char[index]  = char\n",
    "                        index+=1\n",
    "        \n",
    "        for char in self.SPECIAL_CHARS:\n",
    "#             print(\"this is the char\")\n",
    "            if char not in self.char2index:\n",
    "                self.char2index[char] = index\n",
    "                self.index2char[index] = char\n",
    "                index += 1\n",
    "        \n",
    "        print(self.char2index[None])\n",
    "        # potentially need to add 2 for the None and \\n characters\n",
    "        self.vocab_size = len(self.char2index)\n",
    "    \n",
    "    # this allows the model to handle all possible strings passed into it!!\n",
    "    def full_vocab(self):\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "        index = 0\n",
    "        all_letters = string.printable \n",
    "        for char in all_letters:\n",
    "            if char not in self.char2index:\n",
    "                self.char2index[char] = index\n",
    "                self.index2char[index]  = char\n",
    "                index+=1\n",
    "                \n",
    "        for char in self.SPECIAL_CHARS:\n",
    "#             print(\"this is the char\")\n",
    "            if char not in self.char2index:\n",
    "                self.char2index[char] = index\n",
    "                self.index2char[index] = char\n",
    "                index += 1\n",
    "                \n",
    "        self.vocab_size = len(self.char2index)   \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def generate_tensor_for_char(self, char):\n",
    "        temp = torch.zeros(1, self.vocab_size)\n",
    "        temp[0][self.char2index[char]] = 1 \n",
    "        return temp\n",
    "\n",
    "#     def generate_char_from_tensor(self, char):\n",
    "#         temp = torch.zeros(self.vocab_size, 1)\n",
    "#         temp[self.char2index[char]][0] = 1 \n",
    "#         return temp\n",
    "    def lineToTensor(self,line):\n",
    "        tensor = torch.zeros(len(line), 1, self.vocab_size)\n",
    "#         print(line)\n",
    "        for li, letter in enumerate(line):\n",
    "            tensor[li][0][self.char2index[letter]] = 1\n",
    "        return tensor\n",
    "    \n",
    "    # wrap the lines!\n",
    "    def train_example_builder(self, line):\n",
    "        tensor = torch.zeros(len(line), 1, self.vocab_size)\n",
    "        for li, letter in enumerate(line):\n",
    "            tensor[li][0][self.char2index[letter]] = 1\n",
    "        return tensor\n",
    "      \n",
    "#     could do more of a common build api\n",
    "    def get_None_tensor(self):\n",
    "        tensor = torch.zeros(1, 1, self.vocab_size)\n",
    "        tensor[0][0][self.char2index[None]] = 1\n",
    "        return tensor\n",
    "    \n",
    "    def list_to_tensor(self, input_list):\n",
    "        tensor = torch.zeros(len(input_list), 1, self.vocab_size)\n",
    "        for elt in input_list:\n",
    "            tensor[li][0][self.char2index[letter]] = 1\n",
    "            \n",
    "#     def get_training_pair(self, line):\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_new_line_tensor(self):\n",
    "        tensor = torch.zeros(1, 1, self.vocab_size)\n",
    "        tensor[0][0][self.char2index[\"\\n\"]] = 1\n",
    "        return tensor\n",
    "\n",
    "        \n",
    "#         tensor[li][0][self.char2index[letter]] \n",
    "#         temp \n",
    "    def unicodeToAscii(self,s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in self.char2index\n",
    "        )\n",
    "\n",
    "    def __init__(self, text_file,  convert_to_ascii = True):\n",
    "        self.string = string\n",
    "        self.text_file = text_file\n",
    "\n",
    "        self.training_examples = []\n",
    "\n",
    "        total_inputs = []\n",
    "        total_outputs = []\n",
    "        \n",
    "#         make the vocab\n",
    "        self.full_vocab()\n",
    "        \n",
    "        with open(text_file) as file:\n",
    "            for raw_line in file:\n",
    "                \n",
    "                line = self.unicodeToAscii(raw_line) if convert_to_ascii else raw_line\n",
    "                # zero pad to start\n",
    "                \n",
    "                \n",
    "#                 think about how to make this a vector or tensor line! \n",
    "#                 print()\n",
    "                \n",
    "#                 inputs = self.get_None_tensor() + line_to_tensor(line)\n",
    "#                 inputs = [None] + [x for x in line]\n",
    "                inputs = self.lineToTensor([None] + [x for x in line])\n",
    "                \n",
    "                targets = self.lineToTensor([x for x in line] + [\"\\n\"])  # we need a 0 as well!\n",
    "\n",
    "                # append: will be a list of list (of lines)\n",
    "                # extend: will be a list of lines\n",
    "                total_inputs.append(inputs)\n",
    "                total_outputs.append(targets)\n",
    "\n",
    "        assert len(total_inputs) == len(total_outputs)\n",
    "        \n",
    "        # this is now a LIST of lists!\n",
    "        self.total_examples = list(zip(total_inputs, total_outputs))\n",
    "        \n",
    "        #                 for char in line:\n",
    "        # how many chars before the target? or all chars before the target?\n",
    "        # so we simply pair up x with [x+1] i think\n",
    "        # ok, and then we iterate them in the loader?\n",
    "        #                     inputs = [x for x in ]\n",
    "        #                     targets = []\n",
    "\n",
    "        # we should also have some big list of entries \n",
    "\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # ok, now let us just try adding the transforms\n",
    "    # actually the transforms should take responsibility for all of the vocab stuff\n",
    "    \n",
    "    # number of items in the dataset. This involves a rather large computation!\n",
    "    # we can either do a feed forward cllm in the style of bengio, or we can just \n",
    "    # do the RNN approach (as specified by Sean Robertson in his tutorial)\n",
    "\n",
    "    # the indices should represent examples inside the list\n",
    "    # one strange thing is the following: we need to precompute al the indices earlier, imo, and then simply return the list\n",
    "    # at that element! (perhaps with some transforms!)\n",
    "    def __getitem__(self, index):\n",
    "        return self.total_examples[index]\n",
    "\n",
    "        # build a list, return elements from that list? we need to internally keep track of some index\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# ok, so it works now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "abc = TextDataset(os.path.join(\"data\", \"names\", \"English.txt\"), False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(abc.char2index[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.generate_tensor_for_char(\"a\")\n",
    "# abc.index2char[2]\n",
    "abc.lineToTensor(\"abcd\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for (a,b) in abc:\n",
    "    pass\n",
    "#     this should give two lines/tests\n",
    "#     print(a,b)\n",
    "#     print()\n",
    "    \n",
    "#     realistically, we should also have it as tensors/vectors\n",
    "# finally, we should also have some way where we run the rnn forward and \n",
    "#  iteratoring over the set, does not consume the elements! It is not a genertor! Rather, we simply index into the elements we would like\n",
    "for(a,b) in abc:\n",
    "    pass\n",
    "#     print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_x = [None] + [\"e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok, so now let's just do the stuff\n",
    "# the RNN actually makes one-character over predictions! \n",
    "# rnn_forward is a utility function that does a lot of stuff\n",
    "len(char_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, so now we can start building the network that will process these!!\n",
    "# simply: iterate the dataset, and run train on it, do the log likelihood and etc.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "n_hidden = 128\n",
    "\n",
    "\n",
    "rnn = RNN(abc.vocab_size, n_hidden, abc.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.lineToTensor(\"we\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WE NEED THIS LINE TO RESET THE RNN!!\n",
    "rnn = RNN(abc.vocab_size, n_hidden, abc.vocab_size)\n",
    "learning_rate = 0.0005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "#  we had to set it a big lower to force convergence\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(rnn.parameters(), lr = 0.01)\n",
    "\n",
    "# returns the loss for a line\n",
    "\n",
    "def train(input_tensor, target_tensor):\n",
    "    \n",
    "        \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#     print (input_tensor)\n",
    "    \n",
    "#     print(\"changed3\")\n",
    "#     print(input_tensor[1])\n",
    "    criterion = nn.NLLLoss()    \n",
    "    hidden = rnn.initHidden()\n",
    "    loss = 0\n",
    "    rnn.zero_grad()\n",
    "#     print(input_tensor.size())\n",
    "#     print(target_tensor[0])\n",
    "#     print(target_tensor)\n",
    "    for i in range(input_tensor.size()[0]):\n",
    "        output, hidden = rnn(input_tensor[i], hidden)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         find where the one is:\n",
    "# \n",
    "#         torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        max_tensor = torch.argmax(target_tensor[i])\n",
    "#         print(max_tensor)\n",
    "        ind = (torch.argmax(target_tensor[i])).item()\n",
    "#         print(\"this the target\")\n",
    "#         print(target_tensor[i])\n",
    "#         print(ind)\n",
    "        true_target_tensor = torch.tensor([ind], dtype=torch.long)\n",
    "    \n",
    "#     Evaluate the loss on each character!\n",
    "        loss += criterion(output, true_target_tensor)\n",
    "        \n",
    "#         expects JUST a single target class:\n",
    "\n",
    "#     print (\"loss for i \" + str(loss.item()))    \n",
    "#         print()\n",
    "# should be do the loss on each thing, or each batch! no, we should accumulate it! \n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "#     for p in rnn.parameters():\n",
    "#         p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "# why not run this just on all the tensors!! \n",
    "\n",
    "# then, we just call it with the dataset class, and then just iterate everything!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.index2char[0]\n",
    "abc.index2char[1]\n",
    "\n",
    "abc.char2index[\"A\"]\n",
    "\n",
    "# ok some weird stuff here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x,y) in abc:\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3668/3668 [00:13<00:00, 262.72it/s]\n",
      "  1%|          | 29/3668 [00:00<00:12, 285.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss is 38.76116996569564\n",
      "per character loss is 4.615893555814826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3668/3668 [00:14<00:00, 261.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss is 38.76116996569564\n",
      "per character loss is 4.615893555814826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# del rnn\n",
    "rnn = RNN(abc.vocab_size, n_hidden, abc.vocab_size)\n",
    "\n",
    "epoch_loss = 0\n",
    "# loss every k iters\n",
    "# \n",
    "total_loss = 0\n",
    "total_length  = 0\n",
    "epoch_length = 0\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "for k in range(num_epochs ):\n",
    "    \n",
    "    for i,(x,y) in enumerate(tqdm(abc)):\n",
    "    #     print(x)\n",
    "    #     print(x)\n",
    "    #     print(y)\n",
    "        _, loss = train(x,y)\n",
    "        epoch_loss += loss\n",
    "        epoch_length += x.size()[0]\n",
    "        total_loss += loss\n",
    "        total_length += x.size()[0]\n",
    "\n",
    "    #     print(i)\n",
    "    #     abc = i\n",
    "#         if i % 100 == 0:\n",
    "#             print (\"loss for {} is {}\".format(i,other_loss/100))\n",
    "#             other_loss = 0\n",
    "    \n",
    "    print(\"epoch {} loss is {}\".format(k, epoch_loss/i))\n",
    "    print(\"per character loss is {}\".format(epoch_loss/epoch_length))\n",
    "    epoch_loss = 0\n",
    "    epoch_length = 0\n",
    "#     ideally: we would be able to compute the quintile losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the entire model under the given filename\n",
    "def save_model(model,filename):\n",
    "    torch.save(model, filename)\n",
    "    \n",
    "    return\n",
    "\n",
    "# Saves the model parameters only (for maximum usability)\n",
    "\n",
    "def save_model_params(model, filename):\n",
    "\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "    return filename\n",
    "\n",
    "#  ideally, I could get per character loss, then I would also be able to get the total expected loss\n",
    "# so: we want to get total loss, then divide it by all the length of the number of examples we saw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_params(filename):\n",
    "    model = RNN(abc.vocab_size, n_hidden, abc.vocab_size)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model #vs return model.eval()??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"draft_TEST.pt\"\n",
    "\n",
    "if True:\n",
    "\n",
    "    filename = save_model_params(rnn, \"draft_TEST.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (i2h): Linear(in_features=229, out_features=128, bias=True)\n",
       "  (i2o): Linear(in_features=229, out_features=101, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model = load_model_from_params(filename)\n",
    "eval_model.eval() # this is a mutating operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# returns the loss for the entire line (requires normalization for comparison)\n",
    "def get_loss_on_line(rnn, line):\n",
    "    input_tensor =  abc.lineToTensor([None] + [x for x in line])\n",
    "    target_tensor = abc.lineToTensor([x for x in line] + [\"\\n\"])\n",
    "    criterion = nn.NLLLoss()    \n",
    "    hidden = rnn.initHidden()\n",
    "    loss = 0\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    \n",
    "#     print(input_tensor.size())\n",
    "#     print(target_tensor[0])\n",
    "#     print(target_tensor)\n",
    "    for i in range(input_tensor.size()[0]):\n",
    "        output, hidden = rnn(input_tensor[i], hidden)\n",
    "        \n",
    "        \n",
    "        # had to do this simply because of how the NLLL and CrossEntropy are defined\n",
    "        max_tensor = torch.argmax(target_tensor[i])\n",
    "        ind = (torch.argmax(target_tensor[i])).item()\n",
    "        true_target_tensor = torch.tensor([ind], dtype=torch.long)\n",
    "    \n",
    "#     Evaluate the loss on each character!\n",
    "        loss += criterion(output, true_target_tensor)\n",
    "    \n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.568442153930664\n"
     ]
    }
   ],
   "source": [
    "# out = eval_model)\n",
    "\n",
    "# so there's a couple possibilities: we can have jim and ask to predict as well as just get the loss of the example\n",
    "# out = train()\n",
    "# ()\n",
    "#lineToTensor\n",
    "\n",
    "# get it as tensors, then run it, then do loss addition\n",
    "\n",
    "STRING_TO_TEST =  \"David\"\n",
    "print(get_loss_on_line(eval_model, STRING_TO_TEST )/len(STRING_TO_TEST ))\n",
    "\n",
    "# we now want to implement sampling:\n",
    "#  is that useful though? \n",
    "# will it be good?\n",
    "# instead, we can also try and get text for the inputs \n",
    "#  then train on that, then run it on some numerical numbers\n",
    "\n",
    "# visualization would be super useful \n",
    "# for pretty printing, we can colour the terminal output\n",
    "# otherwise, we will need to somehow encode the colour so that it is output to the file\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  print(\"\\x1b[31m\\\"red\\\"\\x1b[0m\")\n",
    "\n",
    "# if no thresholding, then we should at least like somehow graphically display?\n",
    "# no,... this is a different concern. Don't put frontend and backend together! (or visualization and etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mBilly\n",
      "\n",
      "loss2.187842686971029\n",
      "\u001b[34mJoe\n",
      "\n",
      "loss2.927530288696289\n",
      "\u001b[34mBob\n",
      "\n",
      "loss3.550493001937866\n",
      "\u001b[34mThornton\n",
      "\n",
      "loss1.9199905395507812\n",
      "\u001b[34mElliot\n",
      "\n",
      "loss2.500835418701172\n",
      "\u001b[34mRandom\n",
      "\n",
      "loss2.389070783342634\n",
      "\u001b[34mderivative\n",
      "\n",
      "loss3.6175897771661933\n",
      "\u001b[31m';3[39\n",
      "\n",
      "loss8.677899496895927\n",
      "\u001b[31mj0hn_person1\n",
      "\n",
      "loss5.2926177978515625\n",
      "\u001b[34mEnglish\n",
      "\n",
      "loss2.915396213531494\n",
      "\u001b[31mwkijkq\n",
      "\n",
      "loss6.433131626674107\n",
      "\u001b[34mWord\n",
      "\n",
      "loss1.381985855102539\n",
      "\u001b[31mAdditional_names\n",
      "\n",
      "loss4.289423325482537\n",
      "\u001b[31mQuixotic\n",
      "\n",
      "loss4.087853749593099\n",
      "\u001b[31m24uirfejkfk\n",
      "\n",
      "loss6.100430170694987\n",
      "\u001b[34mJavier\n",
      "\n",
      "loss2.5835887363978793\n",
      "\u001b[34mJason\n",
      "\n",
      "loss2.2196717262268066\n",
      "\u001b[31mJiao\n",
      "\n",
      "loss4.139523696899414\n",
      "\u001b[34mChen\n",
      "\n",
      "loss2.830333137512207\n",
      "\u001b[34mAkshay\n",
      "\n",
      "loss2.998241424560547\n",
      "\u001b[34mMbaay\n",
      "\n",
      "loss3.496740976969401\n",
      "\u001b[34mGovind\n",
      "loss3.070594151814779\n"
     ]
    }
   ],
   "source": [
    "# print(Fore.RED + \"yoo\")\n",
    "\n",
    "# print(Fore.BLUE + \"haha\")\n",
    "\n",
    "# if on average more than twice as high loss on this sample, then we should flag it for further review\n",
    "THRESHOLD =  1.8332151545300774 * 2\n",
    "\n",
    "\n",
    "# some more powerful model: uses LSTM as well as additional fields and operators!\n",
    "# ok, so now let us just iterate the text file\n",
    "with open(os.path.join(\"data\",\"test-cases.txt\"), \"r\") as file:\n",
    "    for line in file:\n",
    "        example_loss = get_loss_on_line(eval_model, line)/len(line )\n",
    "        if (example_loss > THRESHOLD):\n",
    "            print(Fore.RED + line)\n",
    "        else:\n",
    "            print(Fore.BLUE + line)\n",
    "        \n",
    "        print(\"loss\" + str(example_loss))\n",
    "    \n",
    "# perhaps we should train on google news corpus for a little while..\n",
    "# this \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anaconda3  ec2_final\t    nltk_data\t\t    sandbox.ipynb\n",
      "cllm_rnn   environment.yml  reuters_news_10000.txt  TextDataset.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls /home/ubuntu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")\n",
    "\n",
    "# sentences = reuters.raw().splitlines()\n",
    "# print(reuters.raw())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(type(sentences))\n",
    "    print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# these are not exactly tokenized on sentences; but that is OK!\n",
    "# sentences[:100]\n",
    "if False:\n",
    "    with open(\"reuters_news.txt\", \"w\") as file:\n",
    "        file.write(reuters.raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a smaller dataset for development\n",
    "# so, let's cut up the reuters into smaller (for example, load just the first K bytes of the file.)\n",
    "import time\n",
    "\n",
    "def make_proto_dataset(size):\n",
    "    with open(\"reuters_news_{}.txt\".format(size), \"w\") as file:\n",
    "        \n",
    "        \n",
    "        file.write('\\n'.join(reuters.raw().splitlines()[0:size]))\n",
    "    print(\"file created with {} lines\".format(size))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     the question now is how many neurons and the architecture to use\n",
    "# we can also look into enhancement with the tqdm inside the object?\n",
    "\n",
    "if False:\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    make_proto_dataset(10000)\n",
    "    elapsed = time.time() - start\n",
    "    print(\"elapsed time is {}\".format(elapsed ))\n",
    "\n",
    "# jeez, so it takes more than 8 minutes to create the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time is 6.738259315490723\n"
     ]
    }
   ],
   "source": [
    "# reuters dataset has umlauts and other non-standard characters, so we should try and account for that! \n",
    "# also: we should try and do a sort of full merge: merge between the train dataset and test dataset vocabs\n",
    "# other than that, we can just apply a regularizer to both sets, to get out the processed data\n",
    "# for now, let's just apply a regularization\n",
    "import time\n",
    "start = time.time()\n",
    "reuters_dataset = TextDataset( \"reuters_news_10000.txt\", True)\n",
    "elapsed = time.time() - start\n",
    "print(\"elapsed time is {}\".format(elapsed ))\n",
    "\n",
    "# jeez, so it takes more than 8 minutes to create the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(reuters_dataset))\n",
    "from tqdm import tqdm\n",
    "def general_train_function(dataset):\n",
    "# del rnn\n",
    "# rnn = RNN(abc.vocab_size, n_hidden, abc.vocab_size)\n",
    "\n",
    "    epoch_loss = 0\n",
    "    # loss every k iters\n",
    "    # \n",
    "    loss_per_k = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_length  = 0\n",
    "    epoch_length = 0\n",
    "\n",
    "\n",
    "    num_epochs = 1\n",
    "    \n",
    "    for k in range(num_epochs ):\n",
    "\n",
    "        for i,(x,y) in enumerate(tqdm(dataset)):\n",
    "        #     print(x)\n",
    "        #     print(x)\n",
    "        #     print(y)\n",
    "            _, loss = train(x,y)\n",
    "            epoch_loss += loss\n",
    "            epoch_length += x.size()[0]\n",
    "            total_loss += loss\n",
    "            total_length += x.size()[0]\n",
    "            loss_per_k += loss\n",
    "        #     print(i)\n",
    "        #     abc = i\n",
    "        \n",
    "            # for the first 100 iters, print the loss of every line!\n",
    "#             if i < 100:\n",
    "#                 print (\"loss for {} is {}\".format(i,epoch_loss/(i+1)))\n",
    "                \n",
    "            \n",
    "            if i % 100 == 0 and i != 0:\n",
    "                print (\"loss for {} is {}\".format(i,loss_per_k/100))\n",
    "                loss_per_k = 0\n",
    "                \n",
    "\n",
    "        print(\"epoch {} loss is {}\".format(k, epoch_loss/i))\n",
    "        print(\"per character loss is {}\".format(epoch_loss/epoch_length))\n",
    "        epoch_loss = 0\n",
    "        epoch_length = 0\n",
    "#     ideally: we would be able to compute the quintile losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WE NEED THIS LINE TO RESET THE RNN!!\n",
    "rnn = RNN(reuters_dataset.vocab_size, 512, reuters_dataset.vocab_size)\n",
    "rnn = reuters_model\n",
    "learning_rate = 0.0005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "#  we had to set it a big lower to force convergence\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(rnn.parameters(), lr = 0.0001)\n",
    "\n",
    "# returns the loss for a line\n",
    "\n",
    "def general_train(input_tensor, target_tensor):\n",
    "    \n",
    "        \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "#     print (input_tensor)\n",
    "    \n",
    "#     print(\"changed3\")\n",
    "#     print(input_tensor[1])\n",
    "    criterion = nn.NLLLoss()    \n",
    "    hidden = rnn.initHidden()\n",
    "    loss = 0\n",
    "    rnn.zero_grad()\n",
    "#     print(input_tensor.size())\n",
    "#     print(target_tensor[0])\n",
    "#     print(target_tensor)\n",
    "    for i in range(input_tensor.size()[0]):\n",
    "        output, hidden = rnn(input_tensor[i], hidden)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         find where the one is:\n",
    "# \n",
    "#         torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        max_tensor = torch.argmax(target_tensor[i])\n",
    "#         print(max_tensor)\n",
    "        ind = (torch.argmax(target_tensor[i])).item()\n",
    "#         print(\"this the target\")\n",
    "#         print(target_tensor[i])\n",
    "#         print(ind)\n",
    "        true_target_tensor = torch.tensor([ind], dtype=torch.long)\n",
    "    \n",
    "#     Evaluate the loss on each character!\n",
    "        loss += criterion(output, true_target_tensor)\n",
    "        \n",
    "#         expects JUST a single target class:\n",
    "\n",
    "#     print (\"loss for i \" + str(loss.item()))    \n",
    "#         print()\n",
    "# should be do the loss on each thing, or each batch! no, we should accumulate it! \n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "#     for p in rnn.parameters():\n",
    "#         p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "# why not run this just on all the tensors!! \n",
    "\n",
    "# then, we just call it with the dataset class, and then just iterate everything!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 103/10000 [00:03<04:44, 34.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 100 is 251.97255268096924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 204/10000 [00:06<05:10, 31.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 200 is 234.6040403652191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 306/10000 [00:09<05:40, 28.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 300 is 234.4470827102661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 404/10000 [00:13<05:16, 30.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 400 is 212.36662675857545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 505/10000 [00:16<04:24, 35.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 500 is 186.90268845558165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 604/10000 [00:19<05:26, 28.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 600 is 181.76997879981994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 706/10000 [00:22<04:22, 35.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 700 is 177.357438955307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 803/10000 [00:25<05:03, 30.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 800 is 166.35841693878174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 905/10000 [00:28<04:33, 33.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 900 is 188.44637283325196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1003/10000 [00:31<04:46, 31.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1000 is 177.94423122406005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1104/10000 [00:34<04:21, 34.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1100 is 173.52205041885375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1209/10000 [00:37<03:49, 38.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1200 is 167.32943879127504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1306/10000 [00:41<04:57, 29.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1300 is 180.05056740760804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1405/10000 [00:44<04:41, 30.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1400 is 174.26244561195372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1509/10000 [00:47<03:55, 36.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1500 is 163.73068364143373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1611/10000 [00:50<03:15, 42.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1600 is 151.5397739124298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1705/10000 [00:52<03:47, 36.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1700 is 122.43813943862915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1808/10000 [00:55<03:31, 38.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1800 is 156.80732619285584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1905/10000 [00:58<04:41, 28.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 1900 is 165.22236526489257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2010/10000 [01:00<02:43, 48.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2000 is 133.73226264953612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2107/10000 [01:03<03:31, 37.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2100 is 149.16234002113342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2206/10000 [01:06<03:57, 32.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2200 is 144.50692591667175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2303/10000 [01:09<04:01, 31.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2300 is 170.0891673374176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2409/10000 [01:12<03:33, 35.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2400 is 150.84665395736695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2506/10000 [01:15<04:39, 26.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2500 is 176.11941156387329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2603/10000 [01:17<02:40, 46.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2600 is 122.58871170043945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2707/10000 [01:20<03:40, 33.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2700 is 159.29627791404724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2806/10000 [01:23<03:44, 31.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2800 is 165.08562461853026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2904/10000 [01:26<03:41, 32.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 2900 is 153.2923412513733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3007/10000 [01:29<02:53, 40.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3000 is 145.9887975502014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3104/10000 [01:31<03:30, 32.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3100 is 127.42265934944153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3204/10000 [01:34<03:50, 29.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3200 is 165.35585821151733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3306/10000 [01:37<02:38, 42.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3300 is 156.1570071220398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3407/10000 [01:39<02:19, 47.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3400 is 123.9395930480957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3505/10000 [01:43<03:07, 34.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3500 is 159.95651490211486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3605/10000 [01:45<02:44, 38.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3600 is 148.9743718147278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3703/10000 [01:48<03:11, 32.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3700 is 157.09264771461486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3806/10000 [01:51<03:03, 33.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3800 is 158.94425254821778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3908/10000 [01:54<02:36, 38.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 3900 is 149.82100116729737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4006/10000 [01:57<03:16, 30.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4000 is 167.07583345413207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4106/10000 [02:00<02:37, 37.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4100 is 146.9348014640808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4204/10000 [02:03<03:10, 30.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4200 is 164.87777397155762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4303/10000 [02:06<03:07, 30.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4300 is 158.00343052864073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4410/10000 [02:09<02:17, 40.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4400 is 155.53267783164978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4511/10000 [02:12<02:14, 40.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4500 is 146.1393644142151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4604/10000 [02:15<02:55, 30.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4600 is 136.5401222038269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4702/10000 [02:18<02:44, 32.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4700 is 153.1510195827484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4806/10000 [02:21<03:03, 28.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4800 is 156.05402238845826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4908/10000 [02:24<02:22, 35.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 4900 is 142.47391105651855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5004/10000 [02:27<02:57, 28.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5000 is 153.29045412063599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5104/10000 [02:30<03:13, 25.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5100 is 175.31217078208923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5205/10000 [02:33<02:42, 29.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5200 is 144.9515376663208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5304/10000 [02:36<02:40, 29.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5300 is 169.58527488708495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5405/10000 [02:40<02:48, 27.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5400 is 164.11197816848755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5505/10000 [02:43<02:02, 36.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5500 is 147.12920431137084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5620/10000 [02:46<01:42, 42.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5600 is 157.91703706741333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5705/10000 [02:48<02:17, 31.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5700 is 131.21591705322265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5806/10000 [02:51<02:02, 34.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5800 is 159.71687029838563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5905/10000 [02:55<02:23, 28.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 5900 is 161.17619038581847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6005/10000 [02:57<01:49, 36.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6000 is 146.18225639343262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6104/10000 [03:00<02:17, 28.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6100 is 155.05190631866455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6207/10000 [03:04<02:12, 28.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6200 is 175.71546012878417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 6304/10000 [03:07<02:12, 27.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6300 is 160.6230272293091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6407/10000 [03:11<01:55, 31.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6400 is 155.7010053539276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6507/10000 [03:14<01:50, 31.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6500 is 161.15787815093995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6606/10000 [03:17<01:53, 30.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6600 is 155.46380370140076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6704/10000 [03:20<01:46, 31.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6700 is 151.9792933368683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6802/10000 [03:23<01:19, 40.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6800 is 134.36411356925964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6909/10000 [03:25<01:21, 37.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 6900 is 143.44868708610534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7006/10000 [03:28<01:28, 33.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7000 is 143.04400129318236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7105/10000 [03:31<01:43, 28.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7100 is 153.24747950553893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7205/10000 [03:34<01:38, 28.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7200 is 151.20772696495055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7304/10000 [03:38<01:38, 27.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7300 is 168.49185995101928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 7406/10000 [03:41<01:21, 31.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7400 is 159.14152278900147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7506/10000 [03:44<01:10, 35.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7500 is 156.77875894546509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7606/10000 [03:47<01:20, 29.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7600 is 158.7559864616394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7703/10000 [03:50<01:08, 33.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7700 is 150.55570925712584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7806/10000 [03:53<00:57, 38.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7800 is 156.4637714767456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7904/10000 [03:56<00:55, 37.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 7900 is 131.36929181098938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8008/10000 [03:59<00:56, 35.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8000 is 146.28438407421112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8105/10000 [04:02<01:00, 31.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8100 is 150.5341159296036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 8206/10000 [04:05<00:55, 32.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8200 is 157.96224998474122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 8302/10000 [04:08<00:35, 47.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8300 is 136.81310372829438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8408/10000 [04:11<00:45, 34.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8400 is 154.6166722011566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8505/10000 [04:14<00:53, 27.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8500 is 154.27702817440033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 8605/10000 [04:17<00:42, 32.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8600 is 125.94283284187317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8710/10000 [04:20<00:36, 35.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8700 is 138.84300857543946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 8804/10000 [04:22<00:37, 31.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8800 is 133.83032779216765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8903/10000 [04:25<00:37, 29.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 8900 is 126.36097214221954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9009/10000 [04:28<00:26, 36.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9000 is 138.08757016658782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9107/10000 [04:30<00:24, 36.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9100 is 133.97674523353578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9206/10000 [04:34<00:18, 42.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9200 is 153.45337667942047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9308/10000 [04:36<00:14, 46.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9300 is 119.5725048494339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 9408/10000 [04:39<00:15, 37.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9400 is 135.52594708919526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 9503/10000 [04:42<00:16, 29.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9500 is 141.41440217494966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9605/10000 [04:44<00:09, 42.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9600 is 126.10387121677398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9704/10000 [04:48<00:09, 32.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9700 is 155.7234737253189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9802/10000 [04:50<00:05, 38.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9800 is 143.66244150161742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9905/10000 [04:53<00:02, 33.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 9900 is 150.4210810852051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:56<00:00, 33.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss is 156.11923495215504\n",
      "per character loss is 3.3185082371638748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    general_train_function(reuters_dataset)\n",
    "# perhaps: examine the gradient instead!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# returns the loss for a line, evaluating on a BASIC rnn, and using the provided optimizer\n",
    "def prototype_general_train(input_tensor, target_tensor, basic_rnn, optimizer):\n",
    "    \n",
    "        \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    criterion = nn.NLLLoss()    \n",
    "    hidden = rnn.initHidden()\n",
    "    loss = 0\n",
    "    basic_rnn.zero_grad()\n",
    "    for i in range(input_tensor.size()[0]):\n",
    "        output, hidden = basic_rnn(input_tensor[i], hidden)\n",
    "        \n",
    "        max_tensor = torch.argmax(target_tensor[i])\n",
    "        ind = (torch.argmax(target_tensor[i])).item()\n",
    "        true_target_tensor = torch.tensor([ind], dtype=torch.long)\n",
    "        #     Evaluate the loss on each character!\n",
    "        loss += criterion(output, true_target_tensor)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    save_model_params(rnn, \"reuters_model_new.pt\")\n",
    "\n",
    "# an rnn class could save some config info like the below!\n",
    "def load_model_from_params_reuters(filename):\n",
    "    print(\"running\")\n",
    "#     print(torch.load(filename))\n",
    "    \n",
    "#     print(reuters_dataset.vocab_size)\n",
    "    model = RNN(reuters_dataset.vocab_size, 512, reuters_dataset.vocab_size)\n",
    "#     model = RNN(abc.vocab_size, n_hidden, abc.vocab_size)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model\n",
    "\n",
    "# reuters_model = load_model_from_params_reuters(\"reuters_model.pt\")\n",
    "reuters_model = load_model_from_params_reuters(\"reuters_model_new.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=613, out_features=512, bias=True)\n",
      "  (i2o): Linear(in_features=613, out_features=101, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "RNN(\n",
      "  (i2h): Linear(in_features=613, out_features=512, bias=True)\n",
      "  (i2o): Linear(in_features=613, out_features=101, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n",
      "<function load_model_from_params_reuters at 0x7f157135d620>\n"
     ]
    }
   ],
   "source": [
    "print(rnn)\n",
    "print(reuters_model)\n",
    "print(load_model_from_params_reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anyway, now let us try running it line by line on a sample abstract dataset\n",
    "import json\n",
    "import os\n",
    "with open(os.path.join(\"dataXplorer\", \"result.json\")) as file:\n",
    "    data  = json.load(file)\n",
    "\n",
    "\n",
    "for key in data:\n",
    "#     print(key)\n",
    "#     print(data[key])\n",
    "    pass\n",
    "    \n",
    "sample_phrase = \"Hello. You are cool.\"\n",
    "for key in data:\n",
    "    sample_phrase = data[key] \n",
    "    break\n",
    "    \n",
    "    \n",
    "# now we just need to cut up the stuff\n",
    "# we can try a tokenizer (for example, nltk tokenizer) \n",
    "# or perhaps we can try moses tokenizer\n",
    "# recall we learnt lots about tokenization at some point: stat tokenizer?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = sent_tokenize(sample_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import reuters\n",
    "# nltk.download('reuters')\n",
    "# reuters.raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The most common differential diagnosis of β-thalassemia (β-thal) trait is iron deficiency anemia.',\n",
       " 'Several red blood cell equations were introduced during different studies for differential diagnosis between β-thal trait and iron deficiency anemia.',\n",
       " 'Due to genetic variations in different regions, these equations cannot be useful in all population.',\n",
       " 'The aim of this study was to determine a native equation with high accuracy for differential diagnosis of β-thal trait and iron deficiency anemia for the Sistan and Baluchestan population by logistic regression analysis.',\n",
       " 'We selected 77 iron deficiency anemia and 100 β-thal trait cases.',\n",
       " 'We used binary logistic regression analysis and determined best equations for probability prediction of β-thal trait against iron deficiency anemia in our population.',\n",
       " 'We compared diagnostic values and receiver operative characteristic (ROC) curve related to this equation and another 10 published equations in discriminating β-thal trait and iron deficiency anemia.',\n",
       " 'The binary logistic regression analysis determined the best equation for best probability prediction of β-thal trait against iron deficiency anemia with area under curve (AUC) 0.998.',\n",
       " 'Based on ROC curves and AUC, Green & King, England & Frazer, and then Sirdah indices, respectively, had the most accuracy after our equation.',\n",
       " 'We suggest that to get the best equation and cut-off in each region, one needs to evaluate specific information of each region, specifically in areas where populations are homogeneous, to provide a specific formula for differentiating between β-thal trait and iron deficiency anemia.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok so this will do what we want then!!\n",
    "\n",
    "# now, it is just a matter of making the dataset then\n",
    "# test sentences:\n",
    "count = 0\n",
    "training_examples = []\n",
    "with open(\"testing_samples.txt\", \"w\") as file:\n",
    "    for key in data:\n",
    "        lines = sent_tokenize(data[key])\n",
    "        for line in lines:\n",
    "            training_examples.append(line)\n",
    "            file.write(\"\\n\")\n",
    "            file.write(line)\n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n"
     ]
    }
   ],
   "source": [
    "# print(len())\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1019/1019 [00:27<00:00, 37.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".]. had loss of 6.08184814453125\n",
      "\n",
      "(1999). had loss of 5.220442635672433\n",
      "\n",
      "(2012) [6]. had loss of 5.006755135276101\n",
      "\n",
      "Klales et al. had loss of 4.939147362342248\n",
      "\n",
      "The Klales et al. had loss of 4.868315752814798\n",
      "\n",
      "Intercept: -4.8092. had loss of 4.853640907689145\n",
      "\n",
      "The accuracy was 90.2%. had loss of 4.811034824537194\n",
      "\n",
      "Retrospective outcome research. had loss of 4.7833030454574095\n",
      "\n",
      "If have, but lack of specificity. had loss of 4.763862378669508\n",
      "\n",
      "The ICU mortality rate was 5.8%. had loss of 4.759453296661377\n",
      "\n",
      "The second substituted RTS for GCS. had loss of 4.756594412667411\n",
      "\n",
      "The first included age, sex, and GCS. had loss of 4.748664237357475\n",
      "\n",
      "Community based cross-sectional study. had loss of 4.745619522897821\n",
      "\n",
      "Applications in practice are discussed. had loss of 4.7428831442808495\n",
      "\n",
      "This disparity remains poorly understood. had loss of 4.739071729706555\n",
      "\n",
      "The outcome measure was ICU mortality. had loss of 4.735730622944079\n",
      "\n",
      "Area under the ROC curves was 0.99. had loss of 4.734659685407366\n",
      "\n",
      "(Qual Life Res 23:2883-2888, 2014). had loss of 4.733690970284599\n",
      "\n",
      "This study includes total 106 subjects. had loss of 4.7324699988731975\n",
      "\n",
      "(2012) [6] for external validation. had loss of 4.730604771205357\n",
      "\n",
      "Two models were developed using these data. had loss of 4.727562128111374\n",
      "\n",
      "Seven cancers were in FIGO stage one. had loss of 4.727313892261402\n",
      "\n",
      "The equation's positive predictive value is 90%. had loss of 4.7272993723551435\n",
      "\n",
      "PC detection rate in our study was 43.8%. had loss of 4.726325058355564\n",
      "\n",
      "The area under the ROC curve was 0.993. had loss of 4.72556872245593\n",
      "\n",
      "Patients were examined with CT angiography. had loss of 4.725176789039789\n",
      "\n",
      "Pooling all samples and using Klales' et al. had loss of 4.724639198996804\n",
      "\n",
      "Data sets produced using Monte Carlo simulations. had loss of 4.721772485849809\n",
      "\n",
      "International Journal of Food Microbiology. had loss of 4.720381625863009\n",
      "\n",
      "The path from diet was direct to depression. had loss of 4.718221491033381\n",
      "\n",
      "140 NIDDM patients were included in this study. had loss of 4.71709296043883\n",
      "\n",
      "Initially the data was aggregated into proc genmod. had loss of 4.714730954637714\n",
      "\n",
      "The logistic model shows statistical superiority. had loss of 4.714639469068878\n",
      "\n",
      "A simplified additive scoring system was also developed. had loss of 4.712182726178851\n",
      "\n",
      "The primary variable was the admission to PICU for MV. had loss of 4.710727267795139\n",
      "\n",
      "However, the underlying mechanisms remain unclear. had loss of 4.7099761962890625\n",
      "\n",
      "The percentage of correct prediction was 95.5%. had loss of 4.709663878095911\n",
      "\n",
      "We examined two datasets for illustrative purposes. had loss of 4.70936554553462\n",
      "\n",
      "Therefore, this is a contribution to the SLOSS question. had loss of 4.709110260009766\n",
      "\n",
      "But the logistic equation requires a computer to solve. had loss of 4.709059281782671\n",
      "\n",
      "damnosum s.l.</i> riverine larval habitat estimators. had loss of 4.707820100604363\n",
      "\n",
      "All variables were independent predictors of outcome. had loss of 4.7076004316221995\n",
      "\n",
      "Phenotype (WT or CS) was coded as the dependent variable. had loss of 4.707050524259868\n",
      "\n",
      "The discrimination of the models was assessed by the c index. had loss of 4.705331771100154\n",
      "\n",
      " Pediatric sepsis is a burdensome public health problem. had loss of 4.704804556710379\n",
      "\n",
      "However, neither solves the problems for each other. had loss of 4.704778817983774\n",
      "\n",
      "In the RTS model, C statistics were 0.80 and 0.67. had loss of 4.704551086425782\n",
      "\n",
      "There were 159 benign and 49 malignant masses. had loss of 4.703992428986923\n",
      "\n",
      "Renal function and risk factors were also analyzed. had loss of 4.703602809532016\n",
      "\n",
      "The equation was validated using a cross-validation procedure. had loss of 4.70286609280494\n",
      "\n",
      "We collected arterial and venous gas analysis data. had loss of 4.702129887599571\n",
      "\n",
      "Data were split 50:50 into training:validation datasets. had loss of 4.701267787388393\n",
      "\n",
      "The regression equation was tested as a PC diagnostic tool. had loss of 4.70068359375\n",
      "\n",
      "The 30-day visiting rate when having physical discomfort was 31.0%. had loss of 4.700154318738339\n",
      "\n",
      "Principal component analysis was used to develop models 2-4. had loss of 4.700113932291667\n",
      "\n",
      "The situation was different for the mixed strain culture. had loss of 4.700012742427358\n",
      "\n",
      "The ROC curve was 0.78, indicating a good discrimination power. had loss of 4.699774363684276\n",
      "\n",
      "SEM successfully recovered the generated model structure. had loss of 4.699717203776042\n",
      "\n",
      "Two hundred and seven (4.6%) in-hospital deaths occurred. had loss of 4.6994778817160086\n",
      "\n",
      "The LOO method was used to test the validity of the equations. had loss of 4.6963515743132564\n",
      "\n",
      "Some skeptical ideas regarding this subject are also included. had loss of 4.696268389301915\n",
      "\n",
      "This translates into a 143% projected increase in TKA volume. had loss of 4.695689216988986\n",
      "\n",
      "The autopsy rate was 50.2%, and PTE prevalence was 10.6%. had loss of 4.6953992341694075\n",
      "\n",
      " Composite endpoints are commonplace in biomedical research. had loss of 4.694871520996093\n",
      "\n",
      "The isolates were identified using CHROM agar Candida test. had loss of 4.694736286745233\n",
      "\n",
      " Sex estimation is an integral aspect of biological anthropology. had loss of 4.694611065204327\n",
      "\n",
      "In 212 cases, PTE was the main cause of death (fatal PTE). had loss of 4.693809772359914\n",
      "\n",
      "We looked at re-hospitalization taken from a Medicare database. had loss of 4.693263462611607\n",
      "\n",
      "Data were stratified by assay, cell type, and days in culture. had loss of 4.693129016507056\n",
      "\n",
      "In this cohort study, 800 bus and truck drivers were recruited. had loss of 4.692354232545883\n",
      "\n",
      " Hemorrhagic shock is a common cause of death in emergency rooms. had loss of 4.6923020582932695\n",
      "\n",
      "In 695 cases, the need of MV in the PICU (Y) was 56 (8.1%). had loss of 4.691852634235964\n",
      "\n",
      "Risk of mortality is predicted with a logistic regression equation. had loss of 4.691286001632463\n",
      "\n",
      "The study was conducted between June 2012 and November 2014. had loss of 4.6908421834309895\n",
      "\n",
      "Logistic regression analysis was used to determine the risk factors. had loss of 4.690759546616498\n",
      "\n",
      "The cases with regional lymph node metastasis were excluded. had loss of 4.690459696451823\n",
      "\n",
      "Orthogonal spatial filter eigenvectors were then generated in SAS/GIS. had loss of 4.690374319893973\n",
      "\n",
      "Laboratory parameters were collected from 306 patients with RA. had loss of 4.689917185949901\n",
      "\n",
      "Oral rinse technique was used to detect the salivary candidal carriage. had loss of 4.689527484732614\n",
      "\n",
      "We tested this proportionality for the herb Helianthus tuberosus. had loss of 4.689098182091346\n",
      "\n",
      "The logistic equation constant was also related to the Monod kinetic constants. had loss of 4.688557298877571\n",
      "\n",
      "Clinical data from 222 patients with meningiomas were collected. had loss of 4.688342571258545\n",
      "\n",
      "Each sample was predicted as \"VRF\" or \"non-VRF\" with this cut-point. had loss of 4.688127854291131\n",
      "\n",
      "The medical records of the patients were retrospectively collected. had loss of 4.687117847044076\n",
      "\n",
      "We show how to estimate standard errors for these estimates. had loss of 4.686456298828125\n",
      "\n",
      "The logistic equation should be used to predicate the mortality when possible. had loss of 4.686428363506611\n",
      "\n",
      "Using substitution method, the veracity of the forecasting equation was 82.1%. had loss of 4.68618657038762\n",
      "\n",
      "The purpose of the study was to develop a statistical model for AMD. had loss of 4.685751971076517\n",
      "\n",
      "With regression equation PC probability for any patient was calculated. had loss of 4.685452313490317\n",
      "\n",
      "Pre-operative GFR was measured directly using a radionuclide GFR assay. had loss of 4.685299295774648\n",
      "\n",
      "WHO's multistage cluster sampling design was used for data collection. had loss of 4.684939575195313\n",
      "\n",
      "Cross-validation of the remaining subjects (N = 21) confirmed this accuracy. had loss of 4.684510080437911\n",
      "\n",
      "We enrolled 390 patients in this study, and 22 were lost to follow-up. had loss of 4.684070696149553\n",
      "\n",
      "The optimal cut-off value for the new combined predictive indicator was 0.518. had loss of 4.68374516413762\n",
      "\n",
      "Fitting a conventional logistic regression can then lead to biased estimation. had loss of 4.683432163336338\n",
      "\n",
      "Different approaches exist for the analysis of composite endpoints. had loss of 4.683212052530317\n",
      "\n",
      " Logistic regression is most often used to produce a cardiac operative risk model. had loss of 4.683191811166158\n",
      "\n",
      "And a mathematic model was established on the multivariate discriminatory analysis. had loss of 4.682609833866717\n",
      "\n",
      "The logistic regression model had an area under a c-statistic of 0.69. had loss of 4.682588413783482\n",
      "\n",
      "When applied to a new sample, the equation's positive predictive value is 86%. had loss of 4.682519766000601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reuters_model.eval()\n",
    "THRESHOLD =  1.8332151545300774 * 2\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in reuters_dataset.char2index\n",
    "    )\n",
    "\n",
    "# get the top N stuff\n",
    "# tuple of sentences and their scores\n",
    "\n",
    "sent_values = []\n",
    "# \n",
    "for line in tqdm(training_examples):\n",
    "    \n",
    "    example_loss = get_loss_on_line(eval_model, unicodeToAscii(line))/len(line )\n",
    "#     print(example_loss)\n",
    "    sent_values.append((line, example_loss))\n",
    "    if (example_loss > THRESHOLD):\n",
    "#         print(line)\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "# return the k highest lines\n",
    "most_confusing  = sorted(sent_values, key=lambda x: x[1], reverse=True)\n",
    "for (sent, val) in most_confusing[:100]:\n",
    "    print(\"{} had loss of {}\\n\".format(sent, val))\n",
    "# print(most_confusing[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# some more powerful model: uses LSTM as well as additional fields and operators!\n",
    "# ok, so now let us just iterate the text file\n",
    "# with open(os.path.join(\"data\",\"test-cases.txt\"), \"r\") as file:\n",
    "#     for line in file:\n",
    "#         example_loss = get_loss_on_line(eval_model, line)/len(line )\n",
    "#         if (example_loss > THRESHOLD):\n",
    "#             print(Fore.RED + line)\n",
    "#         else:\n",
    "#             print(Fore.BLUE + line)\n",
    "        \n",
    "#         print(\"loss\" + str(example_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
